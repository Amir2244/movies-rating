# ==============================================================================
# Configuration for Batch Training Service (batch-training-service)
# ==============================================================================
# ------------------------------------------------------------------------------
# Spark Application Configuration
# ------------------------------------------------------------------------------
# Application name as it will appear in Spark UI and logs
spark.app.name=Movie-Recsys-BatchTrainer-HDFS
# Spark master URL.
# For local development/testing: local[*] (use all available cores) or local[N] (use N cores).
# When submitting to Dataproc, this is typically overridden by the cluster manager (YARN).
spark.master.url=spark://spark-master:7077

# Spark configuration for cluster mode
spark.driver.memory=4g
spark.executor.memory=3g
spark.executor.cores=4
spark.cores.max=4
spark.deploy.mode=client
spark.submit.deployMode=client

# Disable dynamic allocation since shuffle service is not enabled
spark.dynamicAllocation.enabled=false

# Shuffle configuration
spark.shuffle.service.enabled=false
spark.shuffle.spill.compress=true
spark.shuffle.compress=true
spark.io.compression.codec=lz4

# Serialization
spark.serializer=org.apache.spark.serializer.KryoSerializer

# Performance tuning
spark.sql.shuffle.partitions=8
spark.default.parallelism=8
spark.memory.fraction=0.6
spark.memory.storageFraction=0.5
# ------------------------------------------------------------------------------
# HDFS Data Input Configuration (Primary data source for ratings)
# ------------------------------------------------------------------------------
# Full HDFS path to the input ratings data (e.g., CSV file).
# On Dataproc, this path will be within the HDFS of the cluster.
# Example: hdfs:///user/your_username/movielens_data/ratings.csv
# Or, if Dataproc's defaultFS is HDFS, you might use relative paths from the user's HDFS home
# or absolute paths like /data/movielens/ratings.csv if that's where you've placed it.
data.input.ratings.hdfs.path=hdfs://namenode:8020/input/ratings.csv
# Path to save the trained ALS model in HDFS
data.output.model.hdfs.path=hdfs://namenode:8020/models/als_model
# ------------------------------------------------------------------------------
# Redis Configuration (for storing computed model factors)
# ------------------------------------------------------------------------------
# Hostname or IP address of your Redis instance (e.g., Memorystore for Redis IP).
redis.host=redis
# Port number for your Redis instance.
redis.port=6379
# Optional: Redis password if authentication is enabled
# redis.password=your_redis_password
# Optional: Redis database index (default is 0)
# redis.database=0
# ------------------------------------------------------------------------------
# MongoDB Configuration (for storing recommendation results)
# ------------------------------------------------------------------------------
# Hostname or IP address of your MongoDB instance
mongodb.host=mongodb
# Port number for your MongoDB instance
mongodb.port=27017
# Database name for storing recommendations
mongodb.database=movie-recommendations
# Collection name for user recommendations
mongodb.collection.recommendations=user-recommendations
# Collection name for data analytics
mongodb.collection.analytics=analytics
# ------------------------------------------------------------------------------
# ALS Model Hyperparameters
# ------------------------------------------------------------------------------
# Number of latent factors to use (rank of the factor matrices).
als.rank=12
# Maximum number of iterations for the ALS algorithm to run.
als.maxIter=15
# Regularization parameter (lambda) to prevent overfitting.
als.regParam=0.1
# Seed for reproducibility of random operations (e.g., initial factors, data splitting).
als.seed=12345
# Whether to treat preferences as implicit (true) or explicit (false, e.g., ratings).
als.implicitPrefs=false
# Alpha parameter, used only if implicitPrefs is true. Controls the baseline confidence in preference observations.
als.alpha=1.0
# Ratio for splitting data into training and test sets (e.g., 0.8 means 80% for training).
als.trainingSplitRatio=0.8
# ------------------------------------------------------------------------------
# Optional: GCP Configuration (Mainly for local testing or specific GCS interactions)
# ------------------------------------------------------------------------------
# GCP Project ID. Often inferred when running on Dataproc, but can be set explicitly.
# gcp.project.userIdValue=your-gcp-project-userIdValue
# Path to the GCP service account keyfile (JSON).
# - CRITICAL for local testing if your Spark job needs to interact with GCP services (like GCS for staging/logs, even if data is HDFS).
# - When running on Dataproc, VMs usually have service account credentials automatically configured,
#   so this might not be needed or can be left blank unless specific cross-project access is required.
# gcp.keyfile.path=/path/to/your/gcp-service-account-key.json
