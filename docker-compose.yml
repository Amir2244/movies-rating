services:
  namenode:
    image: bde2020/hadoop-namenode
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - ./docker-volumes/namenode_data:/hadoop/dfs/name
    networks:
      - app-network

  datanode:
    image: bde2020/hadoop-datanode
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - namenode
    ports:
      - "9864:9864"
    volumes:
      - ./docker-volumes/datanode_data:/hadoop/dfs/data
    networks:
      - app-network

  spark-master:
    image: bde2020/spark-master
    container_name: spark-master
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./batch-processing-service/target/batch-processing-service-0.0.1.jar:/opt/spark-apps/batch-processing-service.jar
    networks:
      - app-network

  spark-worker:
    image: bde2020/spark-worker
    container_name: spark-worker
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - spark-master
      - datanode
      - redis
    networks:
      - app-network

  spark-worker-2:
    image: bde2020/spark-worker
    container_name: spark-worker-2
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - spark-master
      - datanode
      - redis
    networks:
      - app-network

  kafka:
    image: bitnami/kafka
    container_name: kafka
    ports:
      - "9094:9094"
    volumes:
      - kafka_data:/bitnami/kafka
    networks:
      - app-network
    environment:

      - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:9094,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:9094
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER


      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093


      - KAFKA_CREATE_TOPICS=user_interactions:1:1

  jobmanager:
    image: flink:1.17.1-scala_2.12-java11
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
    networks:
      - app-network

  taskmanager:
    image: flink:1.17.1-scala_2.12-java11
    depends_on:
      - jobmanager
      - kafka
      - redis
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 2
    networks:
      - app-network


  real-time-service:
    build:
      context: ./real-time-service
      dockerfile: Dockerfile
    depends_on:
      - jobmanager
    command: >
      bash -c "
        echo 'Waiting for JobManager to be ready...'
        while ! nc -z jobmanager 8081; do
          sleep 1;
        done;

        echo 'JobManager port is open. Attempting to submit Flink job...'

        MAX_RETRIES=5
        RETRY_COUNT=0
        until flink run -m jobmanager:8081 /opt/flink/usrlib/real-time-service.jar; do
          RETRY_COUNT=$((RETRY_COUNT+1))
          if [ ${RETRY_COUNT} -ge ${MAX_RETRIES} ]; then
            echo 'Failed to submit Flink job after ${MAX_RETRIES} attempts. Exiting.'
            exit 1
          fi
          echo 'Job submission failed, retrying in 5 seconds... (Attempt ${RETRY_COUNT}/${MAX_RETRIES})'
          sleep 5
        done

        echo 'Flink job submitted successfully.'
      "
    networks:
      - app-network

  redis:
    image: redis/redis-stack:latest
    container_name: redis
    ports:
      - "6379:6379"
      - "8001:8001"
    environment:
      - REDIS_ARGS=--save 60 1 --appendonly yes --tcp-keepalive 300 --protected-mode no
    volumes:
      - ./docker-volumes/redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - app-network


  redis-init:
    image: redis/redis-stack:latest
    container_name: redis-init
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./redis-config:/scripts:ro
    command: >
      sh -c "
        echo 'Waiting for Redis to be ready...' &&
        until redis-cli -h redis ping; do
          echo 'Redis is unavailable - sleeping' &&
          sleep 1
        done &&
        echo 'Redis is ready! Initializing vector database...' &&

        echo 'Creating user factors vector index...' &&
        redis-cli -h redis FT.CREATE user_factors_idx ON HASH PREFIX 1 'vector:user:' SCHEMA userId NUMERIC SORTABLE vector VECTOR FLAT 6 TYPE FLOAT32 DIM 50 DISTANCE_METRIC COSINE timestamp NUMERIC SORTABLE modelVersion TAG &&

        echo 'Creating item factors vector index...' &&
        redis-cli -h redis FT.CREATE item_factors_idx ON HASH PREFIX 1 'vector:item:' SCHEMA itemId NUMERIC SORTABLE vector VECTOR FLAT 6 TYPE FLOAT32 DIM 50 DISTANCE_METRIC COSINE timestamp NUMERIC SORTABLE modelVersion TAG &&

        echo 'Creating movie metadata index...' &&
        redis-cli -h redis FT.CREATE movie_metadata_idx ON HASH PREFIX 1 'movie:' SCHEMA movieId NUMERIC SORTABLE title TEXT SORTABLE genres TAG SEPARATOR '|' year NUMERIC SORTABLE &&

        echo 'Creating user recommendations cache index...' &&
        redis-cli -h redis FT.CREATE user_recommendations_idx ON HASH PREFIX 1 'user_rec:' SCHEMA userId NUMERIC SORTABLE movieId NUMERIC SORTABLE rating NUMERIC SORTABLE timestamp NUMERIC SORTABLE modelVersion TAG &&

        echo 'Vector database initialization completed!' &&
        redis-cli -h redis FT._LIST
      "
    restart: "no"
    networks:
      - app-network

  mongodb:
    image: mongo:latest
    container_name: mongodb
    ports:
      - "27018:27017"
    volumes:
      - ./docker-volumes/mongo_data:/data/db
    networks:
      - app-network

volumes:
  namenode_data:
  datanode_data:
  redis_data:
  mongo_data:
  kafka_data:

networks:
  app-network:
    driver: bridge
