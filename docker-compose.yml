services:
  namenode:
    image: bde2020/hadoop-namenode
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - ./docker-volumes/namenode_data:/hadoop/dfs/name
    networks:
      - app-network

  datanode:
    image: bde2020/hadoop-datanode
    container_name: datanode
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - namenode
    ports:
      - "9864:9864"
    volumes:
      - ./docker-volumes/datanode_data:/hadoop/dfs/data
    networks:
      - app-network

  data-importer:
    image: bde2020/hadoop-namenode
    container_name: data-importer
    depends_on:
      - namenode
      - datanode
    entrypoint: /bin/bash
    command: >
      -c "
        set -e;
  
        echo 'Waiting for HDFS to become available...';
        until hdfs dfs -ls /; do
          sleep 5;
        done;
      
        # KEY FIX: Wait for the NameNode to exit Safe Mode
        echo 'Waiting for NameNode to exit Safe Mode...';
        until hdfs dfsadmin -Dfs.defaultFS=hdfs://namenode:8020 -safemode get | grep -q 'Safe mode is OFF'; do
          sleep 5;
        done;
        echo 'NameNode has exited Safe Mode. Proceeding with data import.';
  
        echo 'HDFS is ready. Image is old, updating package sources to archive...';
        sed -i 's/deb.debian.org/archive.debian.org/g' /etc/apt/sources.list;
        sed -i 's/security.debian.org/archive.debian.org/g' /etc/apt/sources.list;
        sed -i '/stretch-updates/d' /etc/apt/sources.list;
      
        echo 'Installing dependencies from archive...';
        apt-get update && apt-get install -y wget unzip;
      
        echo 'Dependencies installed successfully.';
        echo 'Downloading MovieLens ml-latest 33M ratings dataset...';
        wget https://files.grouplens.org/datasets/movielens/ml-latest.zip -O /tmp/ml-latest.zip;
      
        echo 'Extracting dataset...';
        unzip /tmp/ml-latest.zip -d /tmp/;
      
        echo 'Creating /input directory in HDFS...';
        hdfs dfs -Dfs.defaultFS=hdfs://namenode:8020 -mkdir -p /input;
      
        echo 'Copying data files to HDFS...';
        hdfs dfs -Dfs.defaultFS=hdfs://namenode:8020 -put /tmp/ml-latest/*.csv /input/;
      
        echo 'Data import finished successfully! ðŸŽ‰';
      "
    restart: "no"
    networks:
      - app-network
  spark-master:
    image: bde2020/spark-master
    container_name: spark-master
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - app-network

  batch-processing-service:
    build:
      context: ./batch-processing-service
      dockerfile: Dockerfile
    container_name: batch-processing-service
    depends_on:
      - spark-master
      - namenode
      - datanode
      - redis
      - data-importer
    command: >
      bash -c "
        echo 'Waiting for Spark Master to be ready...'
        while ! nc -z spark-master 7077; do
          sleep 1;
        done;

        echo 'Spark Master is ready. Attempting to submit Spark job...'

        MAX_RETRIES=5
        RETRY_COUNT=0
        until /spark/bin/spark-submit --master spark://spark-master:7077 --class org.hiast.batch.launcher.AppLauncher /opt/spark-apps/batch-processing-service.jar analytics; do
          RETRY_COUNT=$((RETRY_COUNT+1))
          if [ ${RETRY_COUNT} -ge ${MAX_RETRIES} ]; then
            echo 'Failed to submit Spark job after ${MAX_RETRIES} attempts. Exiting.'
            exit 1
          fi
          echo 'Job submission failed, retrying in 5 seconds... (Attempt ${RETRY_COUNT}/${MAX_RETRIES})'
          sleep 5
        done

        echo 'Spark job submitted successfully.'
      "
    networks:
      - app-network

  spark-worker:
    image: bde2020/spark-worker
    container_name: spark-worker
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - spark-master
      - datanode
      - redis
    networks:
      - app-network

  spark-worker-2:
    image: bde2020/spark-worker
    container_name: spark-worker-2
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
    depends_on:
      - spark-master
      - datanode
      - redis
    networks:
      - app-network

  kafka:
    image: bitnami/kafka
    container_name: kafka
    ports:
      - "9094:9094"
    volumes:
      - kafka_data:/bitnami/kafka
    networks:
      - app-network
    environment:

      - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:9094,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:9094
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER


      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093


      - KAFKA_CREATE_TOPICS=user_interactions:1:1

  jobmanager:
    image: flink:1.17.1-scala_2.12-java11
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
    networks:
      - app-network

  taskmanager:
    image: flink:1.17.1-scala_2.12-java11
    depends_on:
      - jobmanager
      - kafka
      - redis
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 2
    networks:
      - app-network


  real-time-service:
    build:
      context: ./real-time-service
      dockerfile: Dockerfile
    depends_on:
      - jobmanager
    command: >
      bash -c "
        echo 'Waiting for JobManager to be ready...'
        while ! nc -z jobmanager 8081; do
          sleep 1;
        done;

        echo 'JobManager port is open. Attempting to submit Flink job...'

        MAX_RETRIES=5
        RETRY_COUNT=0
        until flink run -m jobmanager:8081 /opt/flink/usrlib/real-time-service.jar; do
          RETRY_COUNT=$((RETRY_COUNT+1))
          if [ ${RETRY_COUNT} -ge ${MAX_RETRIES} ]; then
            echo 'Failed to submit Flink job after ${MAX_RETRIES} attempts. Exiting.'
            exit 1
          fi
          echo 'Job submission failed, retrying in 5 seconds... (Attempt ${RETRY_COUNT}/${MAX_RETRIES})'
          sleep 5
        done

        echo 'Flink job submitted successfully.'
      "
    networks:
      - app-network

  redis:
    image: redis/redis-stack:latest
    container_name: redis
    ports:
      - "6379:6379"
      - "8001:8001"
    environment:
      - REDIS_ARGS=--save 60 1 --appendonly yes --tcp-keepalive 300 --protected-mode no
    volumes:
      - ./docker-volumes/redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - app-network


  redis-init:
    image: redis/redis-stack:latest
    container_name: redis-init
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./redis-config:/scripts:ro
    command: >
      sh -c "
        echo 'Waiting for Redis to be ready...' &&
        until redis-cli -h redis ping; do
          echo 'Redis is unavailable - sleeping' &&
          sleep 1
        done &&
        echo 'Redis is ready! Initializing vector database...' &&

        echo 'Creating user factors vector index...' &&
        redis-cli -h redis FT.CREATE user_factors_idx ON HASH PREFIX 1 'vector:user:' SCHEMA userId NUMERIC SORTABLE vector VECTOR FLAT 6 TYPE FLOAT32 DIM 50 DISTANCE_METRIC COSINE timestamp NUMERIC SORTABLE modelVersion TAG &&

        echo 'Creating item factors vector index...' &&
        redis-cli -h redis FT.CREATE item_factors_idx ON HASH PREFIX 1 'vector:item:' SCHEMA itemId NUMERIC SORTABLE vector VECTOR FLAT 6 TYPE FLOAT32 DIM 50 DISTANCE_METRIC COSINE timestamp NUMERIC SORTABLE modelVersion TAG &&

        echo 'Creating movie metadata index...' &&
        redis-cli -h redis FT.CREATE movie_metadata_idx ON HASH PREFIX 1 'movie:' SCHEMA movieId NUMERIC SORTABLE title TEXT SORTABLE genres TAG SEPARATOR '|' year NUMERIC SORTABLE &&

        echo 'Creating user recommendations cache index...' &&
        redis-cli -h redis FT.CREATE user_recommendations_idx ON HASH PREFIX 1 'user_rec:' SCHEMA userId NUMERIC SORTABLE movieId NUMERIC SORTABLE rating NUMERIC SORTABLE timestamp NUMERIC SORTABLE modelVersion TAG &&

        echo 'Vector database initialization completed!' &&
        redis-cli -h redis FT._LIST
      "
    restart: "no"
    networks:
      - app-network

  mongodb:
    image: mongo:latest
    container_name: mongodb
    ports:
      - "27018:27017"
    volumes:
      - ./docker-volumes/mongo_data:/data/db
    networks:
      - app-network

  analytics-api:
    build:
      context: ./analytics-api
      dockerfile: Dockerfile
    container_name: analytics-api
    ports:
      - "8083:8083"
    environment:
      - SPRING_DATA_MONGODB_HOST=mongodb
      - SPRING_DATA_MONGODB_PORT=27017
      - SPRING_DATA_MONGODB_DATABASE=movie-recommendations
    depends_on:
      - mongodb
    networks:
      - app-network

  analytics-ui:
    build:
      context: ./analytics-ui
      dockerfile: Dockerfile
    container_name: analytics-ui
    ports:
      - "3000:3000"
    environment:
      - ANALYTICS_API_URL=http://analytics-api:8083
    depends_on:
      - analytics-api
      - recommendations-api
    networks:
      - app-network

  recommendations-api:
    build:
      context: ./recommendations-api
      dockerfile: Dockerfile
    container_name: recommendations-api
    ports:
      - "8082:8080"
    environment:
      - SPRING_DATA_MONGODB_HOST=mongodb
      - SPRING_DATA_MONGODB_PORT=27017
      - SPRING_DATA_MONGODB_DATABASE=movie-recommendations
    depends_on:
      - mongodb
      - kafka
    networks:
      - app-network

volumes:
  namenode_data:
  datanode_data:
  redis_data:
  mongo_data:
  kafka_data:

networks:
  app-network:
    driver: bridge